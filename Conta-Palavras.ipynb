{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools, spacy, stanza\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recursos externos - modelos do spacy e stanza, recursos do nltk\n",
    "nlp_spacy = spacy.load(\"pt_core_news_lg\")\n",
    "stanza.download('pt')\n",
    "nlp_stanza = stanza.Pipeline('pt')\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "stemmer = nltk.stem.RSLPStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dos meus dados\n",
    "Dados = pd.read_csv(\n",
    "    'Corpus  V0- Jornalismo policial e discurso de √≥dio - posts_estudo_inicial.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recorte dos dados para pegar s√≥ os anotados\n",
    "dados1 = Dados[(Dados['Odio_Bol'] == \"1\") | (Dados['Odio_Bol'] == \"0\") | (Dados['Odio_Bol'] == '?') | (Dados['Odio_Bol'] == \"*\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#curiosidade: quantos spam eu anotei?\n",
    "Dados['SPAM?'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explorat√≥ria: como est√£o os dados?\n",
    "print(dados1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contagem bruta da anota√ß√£o pra Discurso de √≥dio\n",
    "dados1.Odio_Bol.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contagem normalizada da anota√ß√£o pra Discurso de √≥dio\n",
    "dados1.Odio_Bol.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contagem bruta da anota√ß√£o pra Discurso de √≥dio em respostas\n",
    "replies = dados1[(dados1['Reply?'] == 1)]\n",
    "replies.Odio_Bol.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contagem normalizada da anota√ß√£o pra Discurso de √≥dio em respostas\n",
    "replies.Odio_Bol.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contagem bruta da anota√ß√£o pra Discurso de √≥dio em posts originais\n",
    "originais =dados1[(dados1['Reply?'] == 0)]\n",
    "originais.Odio_Bol.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contagem normalizada da anota√ß√£o pra Discurso de √≥dio em posts originais\n",
    "originais.Odio_Bol.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extraindo o texto dos tweets de discurso de √≥dio\n",
    "d_odio = dados1[(dados1[\"Odio_Bol\"] == '1') | (dados1['Odio_Bol'] == '?') | (dados1['Odio_Bol'] == \"*\")]\n",
    "tweets_odio = d_odio.tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processamento sem stemming\n",
    "def processa_tweet(tweet):\n",
    "    '''recebe um tweet, elimina users, stopwords'''\n",
    "    lixo = r\"(,|\\.|!|;|:|!|/|\\\\|\\?|\\]|\\}|\\[|\\{|\\-|\\_|\\=|\\+|\\)|\\(|\\#|\\$|\\%|\\&|\\*|\\'|\\))\"\n",
    "    tweet_pont = re.sub(lixo,' ',tweet)\n",
    "    palavras = tweet_pont.split()\n",
    "    copia_limpa = []\n",
    "    for palavra in palavras:\n",
    "        if palavra[0] == '@' or palavra in stopwords:\n",
    "            pass\n",
    "        else:\n",
    "            copia_limpa.append(palavra.lower())\n",
    "            \n",
    "    return copia_limpa\n",
    "\n",
    "alpha = \"@alertario24hrs Claro, ele √© super conivente com.essa zona!\"\n",
    "processa_tweet(alpha)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processa_tweet_nltk(tweet,stemmer):\n",
    "    '''recebe um tweet, elimina users, stopwords'''\n",
    "    lixo = r\"(,|\\.|!|;|:|!|/|\\\\|\\?|\\]|\\}|\\[|\\{|\\-|\\_|\\=|\\+|\\)|\\(|\\#|\\$|\\%|\\&|\\*|\\'|\\))\"\n",
    "    tweet_pont = re.sub(lixo,' ',tweet)\n",
    "    palavras = tweet_pont.split()\n",
    "    copia_limpa = []\n",
    "    for palavra in palavras:\n",
    "        if palavra[0] == '@' or palavra in stopwords:\n",
    "            pass\n",
    "        else:\n",
    "            #com stemmer do nlkt\n",
    "            copia_limpa.append(stemmer.stem(palavra.lower()))\n",
    "    return copia_limpa\n",
    "\n",
    "alpha = \"@alertario24hrs Claro, ele √© super conivente com.essa zona!\"\n",
    "processa_tweet(alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processa_tweet_spacy(tweet,nlp_spacy):\n",
    "    '''recebe um tweet, elimina users, stopwords'''\n",
    "    lixo = r\"(,|\\.|!|;|:|!|/|\\\\|\\?|\\]|\\}|\\[|\\{|\\-|\\_|\\=|\\+|\\)|\\(|\\#|\\$|\\%|\\&|\\*|\\'|\\))\"\n",
    "    tweet_pont = re.sub(lixo,' ',tweet)\n",
    "    tweet_pont = tweet_pont.strip('  ').lower()\n",
    "    copia_limpa = []\n",
    "    for token in nlp_spacy(tweet_pont):\n",
    "        if token.text.startswith('@') or token.text in stopwords:\n",
    "            pass\n",
    "        else:\n",
    "            copia_limpa.append(token.lemma_)\n",
    "    copia_limpa = [x for x in copia_limpa if x not in [' ','   ','  ']]\n",
    "    return copia_limpa\n",
    "\n",
    "alpha = \"@alertario24hrs Claro, ele √© super conivente com.essa zona!\"\n",
    "processa_tweet_spacy(alpha,nlp_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processa_tweet_stanza(tweet,nlp_stanza):\n",
    "    '''recebe um tweet, elimina users, stopwords'''\n",
    "    lixo = r\"(,|\\.|!|;|:|!|/|\\\\|\\?|\\]|\\}|\\[|\\{|\\-|\\_|\\=|\\+|\\)|\\(|\\#|\\$|\\%|\\&|\\*|\\'|\\))\"\n",
    "    tweet_pont = re.sub(lixo,' ',tweet)\n",
    "    tweet_pont = tweet_pont.strip(' ')\n",
    "    #tweet_limpo = ' '.join(tweet_pont)\n",
    "    copia_limpa = []\n",
    "    for sent in nlp_stanza(tweet_pont).sentences:\n",
    "        for word in sent.words:\n",
    "            if word.text.startswith('@') or word.text in stopwords:\n",
    "                pass\n",
    "            else:\n",
    "                copia_limpa.append(word.lemma)\n",
    "    #copia_limpa = [x for x in copia_limpa if x not in [' ','   ','  ']]\n",
    "    return copia_limpa\n",
    "\n",
    "alpha = \"@alertario24hrs Claro, ele √© super conivente com.essa zona!\"\n",
    "processa_tweet_stanza(alpha,nlp_stanza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testa stanza\n",
    "text = \"\"\n",
    "pos = \"\"\n",
    "lemma = []\n",
    "for sent in nlp_stanza(alpha).sentences:\n",
    "    for word in sent.words:\n",
    "        text += word.text + \"\\t\"\n",
    "        pos += word.upos + \"\\t\"\n",
    "        lemma.append(word.lemma)\n",
    "\n",
    "#print(text)\n",
    "#print(pos)\n",
    "print(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testa spacy\n",
    "text = \"\"\n",
    "pos = \"\"\n",
    "lemma = \"\"\n",
    "for token in nlp_spacy(alpha):\n",
    "    text += token.text + \"\\t\"\n",
    "    pos += token.pos_ + \"\\t\"\n",
    "    lemma += token.lemma_ + \"\\t\"\n",
    "    \n",
    "print(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Termos mais frequentes sem stemmer\n",
    "odio_limpo = [processa_tweet(x) for x in tweets_odio]\n",
    "odio_plano = list(itertools.chain(*odio_limpo))\n",
    "\n",
    "counts = Counter(odio_plano)\n",
    "print(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Termos mais frequentes com stemmer NLTK\n",
    "odio_limpo = [processa_tweet_nltk(x,stemmer) for x in tweets_odio]\n",
    "odio_plano = list(itertools.chain(*odio_limpo))\n",
    "\n",
    "counts = Counter(odio_plano)\n",
    "print(counts)\n",
    "#n√£o pega emojis - cria palavras meio incompreens√≠veis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'bandido': 41, 'parab√©m': 25, 'pra': 23, 'pol√≠cia': 19, 'ir': 17, 'ser': 16, 'üëè': 16, 'policiar': 13, 'opera√ß√£o': 13, 'trabalhar': 13, 'matar': 12, 'morrer': 11, 'defender': 11, 'morte': 11, 'armar': 10, 'bem': 10, 'rj': 10, '\"': 10, 'q': 9, 'fazer': 9, 'jacarezinho': 9, 'todo': 9, 'ü§£': 9, 't': 8, 'poder': 8, 'traficante': 8, '24': 8, 'https': 7, 'co': 7, 'morto': 7, 'casar': 7, 'policial': 7, 'querer': 6, 'levar': 6, 'dar': 6, 'direito': 6, 'fam√≠lia': 6, 'mal': 6, 'pessoa': 6, 'dever': 6, 'ladr√£o': 6, 'entrar': 6, 'dessar': 6, 'filho': 6, 'Ô∏è': 6, 'lixar': 5, 'popula√ß√£o': 5, 'ainda': 5, 'cadeia': 5, 'a√≠': 5, '    ': 5, 'civil': 5, 'cima': 5, 'merecer': 5, 'üèΩ': 5, 'ter': 5, 'vagabundar': 5, 'bom': 5, 'dia': 5, 'alertanacional': 5, 'marginar': 5, 'guerreiro': 5, 'ficar': 5, 'ü§¨': 5, 'gente': 4, 'fuzil': 4, 'criminoso': 4, 'militar': 4, 'menos': 4, 'mandar': 4, 'pro': 4, 'ver': 4, 'voltar': 4, 'roubar': 4, 'imprensar': 4, 'sociedade': 4, 'n√©': 4, 'comunidade': 4, 'humano': 4, 'morador': 4, 'precisar': 4, 'pouco': 4, 'errar': 4, 'crime': 4, 'pr√≥prio': 4, 'saber': 4, 'perder': 4, 'pris√£o': 4, 'caro': 4, 'm√£e': 4, 'brasil': 4, 'aqui': 4, '‚ò†': 4, 'vcs': 4, 'bandar': 4, 'excelente': 4, 'atirar': 4, 'parar': 3, 'acontecer': 3, 'seguran√ßa': 3, 'infernar': 3, 'ningu√©m': 3, 'verme': 3, 'homem': 3, 'sim': 3, 'cuidar': 3, 'pobre': 3, 'rir': 3, 'algum': 3, 'favor': 3, 'passar': 3, 'assassino': 3, 'tratar': 3, 'preso': 3, 'solto': 3, 'hoje': 3, 'balir': 3, 'vergonha': 3, 'presar': 3, 'profiss√£o': 3, 'infeliz': 3, 'pr√≥ximo': 3, 'viver': 3, 'deus': 3, 'peno': 3, 'tanto': 3, 'certar': 3, 'pilantras': 3, 'sucesso': 3, 'favela': 3, 'verdadeiro': 3, 'her√≥i': 3, 'apoiar': 3, 'defensor': 3, 'sentimento': 3, 'vagabundo': 3, 'comemorar': 3, 'fardar': 3, 'vir': 3, 'quest√£o': 3, 'pai': 3, 'caralho': 3, 'chegar': 3, 'marmita': 3, 'd√∫vido': 3, 'muito': 2, 'lama': 2, 'ruir': 2, 'covid19': 2, 'apreender': 2, 'a√ß√£o': 2, 'tr√°fico': 2, 'drogar': 2, 'disso': 2, 'justi√ßar': 2, 'm√°ximo': 2, 'assassinar': 2, 'covarde': 2, 'ano': 2, 'maior': 2, 'partir': 2, 'trabalhador': 2, 'total': 2, 'sobrar': 2, 'chance': 2, 'balear': 2, 'hip√≥crita': 2, 'üò°': 2, 'nenhum': 2, 'jacar√©': 2, 'porque': 2, 'vidar': 2, 'chamar': 2, 'dizer': 2, 'cidad√£o': 2, 'preferir': 2, 'assim': 2, 'datena': 2, 'falar': 2, 'outro': 2, 'juiz': 2, 'm√©dico': 2, 'nunca': 2, 'belo': 2, 'pensar': 2, 'interesse': 2, 'frouxo': 2, 'insanidade': 2, 'ent√£o': 2, 'almo': 2, 'crian√ßa': 2, 'marc√£o': 2, 'cidade': 2, 'agora': 2, '11': 2, 'buscar': 2, 'guardar': 2, 'espancar': 2, '40': 2, 'orar': 2, 'falir': 2, 'merda': 2, 'jogar': 2, 'gostar': 2, 'alvejar': 2, 'hj': 2, '2': 2, 'moto': 2, 'cpf': 2, 'cancelar': 2, 'o': 2, 'grande': 2, 'pau': 2, 'tadinhos': 2, 'cada': 2, 'tudo': 2, 'aplaudir': 2, 'enquanto': 2, 'criticar': 2, 'hipocrisia': 2, 'chorar': 2, 'maioria': 2, 'mesmo': 2, 'ontem': 2, 'coisa': 2, 'fdp': 2, 'pm': 2, 'apreendido': 2, 'envolvido': 2, 'lamentar': 2, 'for√ßar': 2, 'triste': 2, 'come√ßar': 2, 'ùóõ√∏ùóΩ': 2, 'pmerj': 2, 'trazer': 2, 'culpar': 2, 'p': 2, 'üèª\\u200d': 2, '‚ôÇ': 2, 'inocente': 2, 'pcerj': 2, '30': 2, 'massacrar': 2, 'possivelmente': 2, 'menino': 2, 'cm': 2, 'tirar': 2, '25': 2, 'embaixo': 1, 'bar': 1, 'lotar': 1, 'fim': 1, 'semana': 1, 'novo': 1, 'igua√ß√∫': 1, 'campe√£o': 1, '     ': 1, 'hxnv9cg1yz': 1, 'acabar': 1, 'atacar': 1, 'armamento': 1, 'guera': 1, 'noite': 1, 'desta': 1, 'sexto': 1, '10': 1, 'borel': 1, 'conhecer': 1, 'ratar': 1, 'atuava': 1, 'resist√™ncia': 1, 'vsfeoxxopp': 1, '1': 1, 'piorar': 1, 'inv√©s': 1, 'sustentar': 1, 'reinar': 1, 'ai': 1, 'socoream': 1, 'mau': 1, 'fazenda': 1, 'bolar': 1, 'ferrar': 1, 'p√©': 1, 'foi√ßar': 1, 'ro√ßar': 1, 'pastar': 1, 'iriar': 1, 'cansar': 1, 'ruim': 1, 'penalidade': 1, 'sujeitar': 1, '18': 1, 'idade': 1, 'jovem': 1, 'adolescente': 1, 'filhar': 1, 'g1': 1, 'narrativo': 1, 'familia': 1, 'invers√£o': 1, 'valorar': 1, 'fato': 1, 'ocorrer': 1, 'entregar': 1, 'perguntar': 1, 'cabe√ßa': 1, '√≥bito': 1, 'ü§î': 1, 'v√≠tima': 1, 'optar': 1, 'aliciador': 1, 'leviano': 1, 'dr': 1, 'alan': 1, 'tinir': 1, 'morar': 1, 'absurdo': 1, 'ocar': 1, 'arriscar': 1, 'vagner': 1, 'monte': 1, 'secret': 1, 'determinar': 1, 'nao': 1, 'pingar': 1, 'considera√ß√£o': 1, 'respeitar': 1, 'vencer': 1, 'crer': 1, 'cad√™': 1, 'balan√ßogeralrj': 1, 'enfrentar': 1, 'beijo': 1, 'secret√°rio': 1, 'assacinado': 1, 'amigar': 1, 'verdade': 1, 'lament√°vel': 1, 'pcrj': 1, 'arrasar': 1, 'janeiro': 1, 'vender': 1, 'corregedoria': 1, 'apurar': 1, 'milhar': 1, 'tb√©m': 1, 'clem√™ncia': 1, 'contra': 1, 'mulher': 1, 'interessante': 1, 'choramingar': 1, 'sentir': 1, 'arsenal': 1, 'canh√£o': 1, 'n√∫mero': 1, 'ftxqjfblko': 1, 'classe': 1, 'crm': 1, 'ca√ßar': 1, 'exercer': 1, 'decente': 1, 'sujar': 1, 'profissional': 1, 'terceiro': 1, 'safar': 1, 'car√°ter': 1, 'ego√≠sta': 1, 'mundo': 1, 'quatro': 1, '9jvzhczd9': 1, 'lei': 1, 'favorecer': 1, 'virar': 1, 'advogar': 1, 'desgra√ßar': 1, 'alegar': 1, 'mental': 1, 'estar': 1, 'protejer√°': 1, 'jaula': 1, 'homossexualismo': 1, 'c√©u': 1, 'exemplo': 1, 'lado': 1, 'cantar': 1, 'sp': 1, 'cometer': 1, 'v√°rio': 1, 'fraudar': 1, 'dinheiro': 1, 'aux√≠lio': 1, 'emergencial': 1, 'cumprir': 1, 'apreens√£o': 1, '70': 1, 'monstro': 1, 'abandonar': 1, 'beber': 1, 'retirar': 1, 'inocentar': 1, 'perto': 1, 'amor': 1, 'dudu': 1, 'camargo': 1, 'pagar': 1, 'atar': 1, 'grade': 1, 'poss√≠vel': 1, 'satanista': 1, 'discursar': 1, 'simplista': 1, 'vaziar': 1, 'disfar√ßar': 1, 'moralista': 1, 'tipo': 1, 'lato': 1, 'hist√≥ria': 1, 'lafgi67c92': 1, 'adhemar': 1, 'servi√ßo': 1, 't√°': 1, 'infestar': 1, 'abc': 1, 'vacinar': 1, 'furar': 1, 'filar': 1, 'exonerar': 1, 'cade': 1, 'judici√°rio': 1, 'eficiente': 1, 'tomar': 1, 'providenciar': 1, 'magistrado': 1, 'soltar': 1, 'politicos': 1, 'rapidinho': 1, 'sikera': 1, 'jr': 1, 'p√¥r': 1, 'mapa': 1, 'prefeito': 1, 'gorver': 1, 'adorar': 1, 'ladr√£e': 1, 'militante': 1, 'lula': 1, 'durante': 1, 'pandemia': 1, 'costa': 1, 'prest': 1, 'bolsonaro': 1, 'motorista': 1, 'atropelar': 1, 'assaltante': 1, 'nele': 1, 'quantum': 1, 'custar': 1, 'aparato': 1, 'log√≠stico': 1, 'cesta': 1, 'b√°sico': 1, 'roto': 1, 'üî™': 1, 'incinerar': 1, 'tro√ßo': 1, 'üò¨': 1, 'üò®': 1, 'puli√ßada': 1, '      ': 1, 'admirar': 1, 'dif√≠cil': 1, 'tentar': 1, 'lixo': 1, 'pgr': 1, 'eliminar': 1, 'canalha': 1, 'sala': 1, 'ar': 1, 'condicionar': 1, 'vagabundagem': 1, 'pedir': 1, 'qdo': 1, '√¥nibus': 1, 'meio': 1, 'd√∫zia': 1, 'v√°rios': 1, 'dentro': 1, 'puta': 1, 'boto': 1, 'nois': 1, 'fogo': 1, 'pesames': 1, '‚Äú': 1, 'rodar': 1, '‚Äù': 1, 'üí™': 1, 'üèº': 1, 'faxinanojacarezinho': 1, 'sentar': 1, 'colar': 1, 'capeta': 1, 'viciar': 1, 'guerra': 1, 'quantidade': 1, 'muni√ß√£o': 1, 'provar': 1, 'protestante': 1, 'parente': 1, 'corpora√ß√£o': 1, 'oficial': 1, 'l√°': 1, 'aben√ßoar': 1, 'pc': 1, 'fant√°stico': 1, 'üáß': 1, 'üá∑': 1, 'voces': 1, 'd√≥': 1, 'esp√©cie': 1, 'apenas': 1, 'obrigar': 1, '30o': 1, 'bp': 1, 'adiantar': 1, 'j√≥': 1, '38': 1, 'gamar': 1, 'over': 1, 'a√ßo‚ÄºÔ∏è': 1, 'fjn6ff8rxk': 1, 'amigo': 1, 'companheiro': 1, 'enlutar': 1, 'atua√ß√£o': 1, 'povo': 1, 'destas': 1, 'protegido': 1, 'bravo': 1, 'agente': 1, 'executar': 1, 'preciso': 1, 'legar': 1, 'combater': 1, 'criminalidade': 1, 'herois': 1, 'euapoioapmerj': 1, 'pesar': 1, 'cora√ß√£o': 1, 'florir': 1, 'brasileiro': 1, 'especificamente': 1, 'carioca': 1, 'achar': 1, 'independente': 1, 'pa√≠s': 1, 'saidinha': 1, 'trancar': 1, 'pular': 1, 'laje': 1, 'temer': 1, 'novamente': 1, 'ohoo': 1, 'üòÆ': 1, 'coitar': 1, 'genocida': 1, 'ü§∑': 1, 'ü§¶': 1, 'sangue': 1, 'azul': 1, 'largo': 1, 'a√ßo': 1, 'globonews': 1, 'du': 1, 'investiga√ß√£ozinha': 1, '‚òÄ': 1, 'n': 1, 'cair': 1, 'tb': 1, '‚Ä¶': 1, 'üßê': 1, 'g√°s': 1, 'lacrimog√™neo': 1, 'spray': 1, 'pimenta': 1, 'resistir': 1, 'detido': 1, 'palha√ßada': 1, 'vtnc': 1, 'ta': 1, 'olhar': 1, 'jeito': 1, 'parecer': 1, 'livro': 1, 'carteiro': 1, 'debaixo': 1, 'bra√ßo': 1, 'esperar': 1, 'acender': 1, 'vela': 1, 'heran√ßa': 1, 'pt': 1, 'qualquer': 1, 'longe': 1, 'pilantra': 1, '√™xito': 1, 'p√™same': 1, 'perda': 1, 'irm√£o': 1, 'tmj': 1, 'engravatar': 1, 'patrocinar': 1, 'utens√≠lio': 1, 'preju√≠zo': 1, 'gde': 1, 'caber': 1, 'descobrir': 1, 'desviar': 1, 'armadilhar': 1, 'pegar': 1, 'infiltrar': 1, 'calhar': 1, 'üëä': 1, 'üèæ': 1, 'quanto': 1, 'deixar': 1, 'paci√™ncia': 1, 'investigar': 1, 'julgar': 1, 'condenar': 1, '1000': 1, 'fornecedor': 1, 'esquerdo': 1, 'cir√∫rgico': 1, 'demais': 1, '50': 1, 'protesto': 1, 'sphf3vefwm': 1, 'amea√ßar': 1, 'antar': 1, 'acreditar': 1, 'obs': 1, 'granar': 1, 'sinal': 1, 'globolixoooo': 1, 'ajudar': 1, 'maria': 1, 'tbm': 1, 'ler': 1, 'nado': 1, 't√£o': 1, 'real': 1, 'queimar': 1, 'tardar': 1, 'anjo': 1, 'mentir': 1, '21': 1, 'recebido': 1, 'primeiro': 1, 'iniciar': 1, 'repelir': 1, 'injusto': 1, 'agress√£o': 1, 'proporcionalidade': 1, 'insano': 1, 'votar': 1, 'neste': 1, 'senhor': 1, 'pena': 1, 'andar': 1, 'escoltar': 1, 'marginal': 1, 'so': 1, 'familiar': 1, 'sofrer': 1, 'apoiopoliciarj': 1, 'algu√©m': 1, 'freixo': 1, 'juntar': 1, 'junto': 1, 'protetor': 1, 'üíÄ': 1, 'üò±': 1, 'horar': 1, 'acertar': 1, 'contar': 1, 'colocar': 1, 'linha': 1, 'frente': 1, 'conversar': 1, 'vc': 1, 'c√∫': 1, 'medo': 1, 'aceitar': 1, 'caridade': 1, 'chuva': 1, 'ir√¥nica': 1, 'pseudo': 1, 'deputar': 1, 'aparecer': 1, 'quemmatoupolicialnojacarezinho': 1, 'üçª': 1, 'üéâ': 1, 'permitir': 1, 'lembrar': 1, 'analista': 1, 'segoran√ßa': 1, 'globolixo': 1, 'depender': 1, 'fichar': 1, 'criminal': 1, 'apesar': 1, 'confirmar': 1, 'capiroto': 1, 'preparar': 1, 'seboso': 1, 'quebrar': 1, 'firmar': 1, 'sair': 1, 'jacar√©zinho': 1})\n"
     ]
    }
   ],
   "source": [
    "#Termos mais frequentes com stemmer Spacy\n",
    "odio_limpo = [processa_tweet_spacy(x,nlp_spacy) for x in tweets_odio]\n",
    "odio_plano = list(itertools.chain(*odio_limpo))\n",
    "\n",
    "counts = Counter(odio_plano)\n",
    "print(counts)\n",
    "#melhor resultado - considera emojis palavras,\n",
    "#apesar de criar um singular para 'parab√©ns' que eu sei l√° de onde tirou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Termos mais frequentes com stemmer Stanza\n",
    "odio_limpo = [processa_tweet_stanza(x,nlp_stanza) for x in tweets_odio]\n",
    "odio_plano = list(itertools.chain(*odio_limpo))\n",
    "\n",
    "counts = Counter(odio_plano)\n",
    "print(counts)\n",
    "\n",
    "### deu ruim com os emojis, mas parece ser bom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.lemma_ for x in nlp('eu gosto de voc√™')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'a meu deus'\n",
    "b = a.split()\n",
    "c = ' '.join(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
